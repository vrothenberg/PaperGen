{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "SEMANTIC_SCHOLAR_API_KEY = os.getenv(\"SEMANTIC_SCHOLAR_API_KEY\")\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreviewComputeTokensResult(tokens_info=[TokensInfo(token_ids=[883, 603, 476, 2121], tokens=[b'this', b' is', b' a', b' test'], role='user')])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vertexai.preview import tokenization\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Vertex AI\n",
    "PROJECT_ID = \"rbio-p-datasharing\"  # Replace with your project ID\n",
    "LOCATION = \"us-central1\"  # Replace with your location\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "model_name = \"gemini-1.5-flash\"\n",
    "text = \"this is a test\"\n",
    "tokenizer = tokenization.get_tokenizer_for_model(model_name)\n",
    "\n",
    "\n",
    "response = tokenizer.compute_tokens(text)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = response.tokens_info[0].token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[883, 603, 476, 2121]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tokens_info[0].token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://us-central1-aiplatform.googleapis.com/v1/projects/rbio-p-datasharing/locations/us-central1/models/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m \u001b[43mdecode_with_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokens_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 27\u001b[0m, in \u001b[0;36mdecode_with_tokenizer\u001b[0;34m(token_ids)\u001b[0m\n\u001b[1;32m     25\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m: [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: token_ids}]}\n\u001b[1;32m     26\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/circadia/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://us-central1-aiplatform.googleapis.com/v1/projects/rbio-p-datasharing/locations/us-central1/models/"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "LOCATION = \"us-central1\"\n",
    "MODEL_NAME = \"gemini-1.5-pro-latest\"\n",
    "\n",
    "def get_access_token():\n",
    "    \"\"\"Gets an access token using gcloud.\"\"\"\n",
    "    import subprocess\n",
    "    try:\n",
    "        output = subprocess.check_output([\"gcloud\", \"auth\", \"print-access-token\"])\n",
    "        return output.strip().decode()\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error getting access token: {e}\")\n",
    "        return None\n",
    "\n",
    "def decode_with_tokenizer(token_ids: list):\n",
    "    \"\"\"Decodes token IDs using the REST API.\"\"\"\n",
    "    url = f\"https://us-central1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/models/\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {get_access_token()}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\"instances\": [{\"token_ids\": token_ids}]}\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"predictions\"][0][\"text\"]\n",
    "\n",
    "decode_with_tokenizer(response.tokens_info[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during encoding: type object 'Endpoint' has no attribute 'find'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def encode_with_tokenizer(text: str, model_name: str):\n",
    "    \"\"\"Encodes text using the Vertex AI API (which uses the model's tokenizer).\"\"\"\n",
    "    try:\n",
    "        endpoint = aiplatform.Endpoint.find(\n",
    "            endpoint=model_name, location=LOCATION\n",
    "        )[0]  # Find the endpoint associated with the model\n",
    "\n",
    "        response = endpoint.predict(instances=[{\"content\": text}])  # Send text to the endpoint\n",
    "        encoded_ids = response.predictions[0][\"token_ids\"]\n",
    "        return encoded_ids\n",
    "    except IndexError:\n",
    "        print(f\"Error: No endpoint found for model '{model_name}'. \"\n",
    "              f\"Make sure the model is deployed or available in your project.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during encoding: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "encode_with_tokenizer(\"test\", model_name=\"gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_with_tokenizer(token_ids: list, model_name: str):\n",
    "    \"\"\"Decodes token IDs using the Vertex AI API.\"\"\"\n",
    "    try:\n",
    "        endpoint = aiplatform.Endpoint.find(\n",
    "            endpoint=model_name, location=LOCATION\n",
    "        )[0]  # Find the endpoint associated with the model\n",
    "\n",
    "        response = endpoint.predict(instances=[{\"token_ids\": token_ids}])\n",
    "        decoded_text = response.predictions[0][\"text\"]\n",
    "        return decoded_text\n",
    "    except IndexError:\n",
    "        print(f\"Error: No endpoint found for model '{model_name}'. \"\n",
    "              f\"Make sure the model is deployed or available in your project.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during decoding: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    original_text = \"This is a test sentence for encoding and decoding with Gemini Flash 1.5.\"\n",
    "    model_name = \"gemini-1.5-flash-001\"\n",
    "\n",
    "    encoded_ids = encode_with_tokenizer(original_text, model_name)\n",
    "\n",
    "    if encoded_ids is not None: #Check if encode was successful\n",
    "        print(f\"Encoded IDs: {encoded_ids}\")\n",
    "\n",
    "        decoded_text = decode_with_tokenizer(encoded_ids, model_name)\n",
    "        if decoded_text is not None: #Check if decode was successful\n",
    "            print(f\"Decoded Text: {decoded_text}\")\n",
    "            if original_text == decoded_text:\n",
    "                print(\"Encode and decode successful! Text matches.\")\n",
    "            else:\n",
    "                print(\"Encode and decode successful but text does not match. This is unusual.\")\n",
    "        else:\n",
    "            print(\"Decoding failed.\")\n",
    "    else:\n",
    "        print(\"Encoding failed.\")\n",
    "\n",
    "    #Test a different model\n",
    "    model_name = \"gemini-pro\"\n",
    "    encoded_ids = encode_with_tokenizer(original_text, model_name)\n",
    "\n",
    "    if encoded_ids is not None: #Check if encode was successful\n",
    "        print(f\"\\nEncoded IDs (Gemini Pro): {encoded_ids}\")\n",
    "\n",
    "        decoded_text = decode_with_tokenizer(encoded_ids, model_name)\n",
    "        if decoded_text is not None: #Check if decode was successful\n",
    "            print(f\"Decoded Text (Gemini Pro): {decoded_text}\")\n",
    "            if original_text == decoded_text:\n",
    "                print(\"Encode and decode successful! Text matches.\")\n",
    "            else:\n",
    "                print(\"Encode and decode successful but text does not match. This is unusual.\")\n",
    "        else:\n",
    "            print(\"Decoding failed.\")\n",
    "    else:\n",
    "        print(\"Encoding failed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circadia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
